 I am an Electrical Engineering Ph.D. candidate at [North Carolina State University](https://www.ncsu.edu/). I am working with [Dr. Edgar Lobaton](http://www.ece.ncsu.edu/people/ejlobato/) at [Active Robotic Sensing (ARoS) Laboratory](http://research.ece.ncsu.edu/aros//) on computer vision and machine learning.
 My current research mainly focuses on robust image segmentation for different applications, including obstacle detection for autonomous car, topology preserving segmentation for learning the shell structure of small organisms and consensus-based natural image segmentation.
 I also like digging deep into various deep learning models through implementing the models and reproducing the experiments from research papers.
 
 
 
 


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td class="col-md-3">May 2019 (expected)</td>
    <td>
        <strong>Ph.D. in Electrical Engineering</strong>
        <br>
      North Carolina State University, Raleigh, NC, USA
    </td>
  </tr>
  <tr>
    <td class="col-md-3">June 2011</td>
    <td>
        <strong>M.S. in Electrical Engineering</strong>
        <br>
      University of Electronic Science and Technology of China, Chengdu, P.R. China
    </td>
  </tr>
  <tr>
    <td class="col-md-3">July 2008</td>
    <td>
        <strong>B.S. in Electrical Engineering</strong>
        <br>
      University of Electronic Science and Technology of China, Chengdu, P.R. China
    </td>
  </tr>
</table>


## <i class="fa fa-chevron-right"></i> Skills
<table class="table table-hover">
<tr>
  <td class='col-md-2'>Computer Languages</td>
  <td markdown="1">
Python, MATLAB, C/C++
  </td>
</tr>
<tr>
  <td class='col-md-2'>Frameworks/Toolbox</td>
  <td markdown="1">
TensorFlow, NumPy, Scikit-learn, OpenCV, Git
  </td>
</tr>
<tr>
  <td class='col-md-2'>Professional</td>
  <td markdown="1">
Computer Vision, Image Segmentation and Classification, Object Detection, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), Visual Attention Models 
  </td>
</tr>
<!-- <tr>
  <td class='col-md-2'>Systems</td>
  <td markdown="1">
Linux, OSX
  </td>
</tr> -->
</table>

## <i class="fa fa-chevron-right"></i>Work Experience
<table class="table table-hover">
  <tr>
    <td class="col-md-3">Jan 2018 -- May 2018<br>
    <a><img src='images/projects/load.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
    <td>
        <strong>Research Aid Intern, <a href='https://www.anl.gov/' target='_blank'>Argonne National Laboratory</a>, Lemont, IL</strong>
        <br>            
	<div>

		Primarily worked on validation of power system models and applying machine learning algorithms to learn load models. Designed a RNN based load demand forecasting model and achievel state-of-the-art performance.
		<br>
		
[<a href='javascript: none' 
    onclick='$("#load").toggle()'>details</a>] <br>
    
<div id="load" style="text-align: justify; display: none" markdown="1">
<ul> 
    <li>Designed a Sequence-to-Sequence-based model for load demand forecasting, and proposed a rich feature learning process to improve accuracy and model interpretability.</li>
    <li>Achieved better or comparable performance to state-of-the-art load demand forecasting models on three public dataset.</li>

</ul>
		<ul> 
<!--			<li>
    			Primarily worked on validation of power load system models and design of load forecasting models. 
    		</li>
    		<li>
    		Designed a Sequence-to-Sequence-based model for load demand forecasting, and proposed a rich feature learning process to improve accuracy and model interpretability.
    		</li>
    		<li>
    		Achieved better or comparable performance to state-of-the-art load demand forecasting models on three public dataset.
    		</li>-->
		</ul>
	</div>
      
    </td>
  </tr>

</table>


## <i class="fa fa-chevron-right"></i>Open Source Projects
<table class="table table-hover">

<tr>
<td class="col-md-3"><a><img src='images/projects/yolo.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
	<strong>YOLOv3 for Object Detection</strong><br>
	<div>
		<ul> 
    		<li>
    			TensorFlow implementation of YOLOv3 object detection for both inference and training.
    		</li>
    		<li>
    			Provided customized training blocks, including bounding box clustering, data augmentation and multi-scale training.
    		</li>
    		<li>
    			<a href='https://github.com/conan7882/YOLOv3/tree/master#train-on-voc2012-dataset-20-classes' target='_blank'>Trained</a> on <a href='http://host.robots.ox.ac.uk/pascal/VOC/' target='_blank'>PASCAL VOC dataset</a> for 20 object classes detection on natural images.
    		</li>
		</ul>
	</div>
	
	[<a href='https://github.com/conan7882/yolov3' target='_blank'>code</a>]<br>
</td>
</tr>

<tr>
<td class="col-md-3"><a><img src='images/projects/gans.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
	<strong>Implementations of Generative Adversarial Networks (GANs)</strong><br>
	<div>
		<ul> 
    		<li>
    			Implementions of various GANs models, including <a href='https://github.com/conan7882/tf-gans/tree/master/docs/dcgan#deep-convolutional-generative-adversarial-networks-dcgan' target='_blank'>DCGAN</a>, <a href='https://github.com/conan7882/tf-gans/tree/master/docs/lsgan' target='_blank'>LSGAN</a> and <a href='https://github.com/conan7882/tf-gans/tree/master/docs/infogan' target='_blank'>InfoGAN</a> for comparison and analyzing the training behaviors of different GANs.
    		</li>
    		<li>
    			Applied on <a href='https://github.com/conan7882/tf-gans#mnist' target='_blank'>MNIST</a> dataset and <a href='https://github.com/conan7882/tf-gans#celeba' target='_blank'>CelebA</a> human face dataset.
    		</li>
<!--     		<li>
    			<a href='https://github.com/conan7882/tf-gans#celeba' target='_blank'>Visualized</a> the interpolation of the learned latent space.
    		</li> -->
    		<li>
    			<a href='https://github.com/conan7882/tf-gans/tree/master/docs/infogan#celeba' target='_blank'>Generated</a> face images with controlled context, such as emotion, hairstyle and azimuth, in an unsupervised manner by using InfoGAN.
    		</li>
		</ul>
	</div>  
	
	[<a href='https://github.com/conan7882/tf-gans' target='_blank'>code</a>] <br>
</td>
</tr>

<tr>
<td class="col-md-3"><a><img src='images/projects/aae.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
	<strong>Adversarial Autoencoders for Variational Inference and Semi-Supervised Learning</strong><br>
	<div>
		<ul> 
    	<li>
    		Provided an implemented of adversarial autoencoders (AAE) which utilize GAN framework as a <a href='https://github.com/conan7882/adversarial-autoencoders#result' target='_blank'>variational inference</a> algorithm.
    	</li>
    	<li>
    		Applied for <a href='https://github.com/conan7882/adversarial-autoencoders#result-3' target='_blank'>semi-supervised learning</a> and <a href='https://github.com/conan7882/adversarial-autoencoders#result-2' target='_blank'>disentangling style and content of images</a>.
    	</li>
		</ul>
	</div>
    
	[<a href='https://github.com/conan7882/adversarial-autoencoders' target='_blank'>code</a>] <br> 
</td>
</tr>

<tr>
<td class="col-md-3"><a><img src='images/projects/pix2pix.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Image to Image Translation with Conditional GANs</strong><br>
    <div>
    <ul>
    <li>
    Reconstructed building facade photos from label maps and generated shoes photos from sketches using pix2pix conditional GANs
    </li> 
    </ul>
    </div>
    
    [<a href='https://github.com/conan7882/pix2pix' target='_blank'>code</a>]<br>
</td>
</tr>
    
<tr>
<td class="col-md-3"><a><img src='images/projects/cnnviz.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Visualization CNN for Interpretation of Trained Models</strong><br>
    <div>
    <ul>
    <li>
    Provided interpretation of trained CNN models by visualizing the learned features and the image regions where the models pay attention to.
    </li> 
    <li>
    <a href='https://github.com/conan7882/CNN-Visualization/blob/master/doc/deconv/README.md#results' target='_blank'>Transposed convolutional network</a> and <a href='https://github.com/conan7882/CNN-Visualization/tree/master/doc/guided_backpropagation#results' target='_blank'>guided back propagation</a> were applied for learned feature visualization.
    </li>
    <li>
    <a href='https://github.com/conan7882/CNN-Visualization/tree/master/doc/cam#results' target='_blank'>Class Activation Mapping</a> and <a href='https://github.com/conan7882/CNN-Visualization/tree/master/doc/grad_cam#results' target='_blank'>Gradient-weighted Class Activation Mapping</a> were applied for attention region visualization.
    </li>
    </ul>
    </div>
	[<a href='http://github.com/conan7882/CNN-Visualization' target='_blank'>code</a>] <br>
</td>
</tr>
	
	
<tr>
<td class="col-md-3"><a><img src='images/projects/ram.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
<strong>Image Classification using Recurrent Attention Model</strong><br>
<div>
<ul> 
    <li>Implemented a recurrent visual attention model for image classification, which reduces the computational complexity by only focusing on a sequence of small regions of the image.
    </li>
    <li>Achieved 97.82% accuracy on 60 x 60 translated MNIST and provided interpretation of the classification results by <a href='https://github.com/conan7882/recurrent-attention-model#result' target='_blank'>visualizing</a> the attention regions during inference.
    </li>
</ul>
</div>
    
[<a href='https://github.com/conan7882/recurrent-attention-model' target='_blank'>code</a>] <br>
    
</td>
</tr>

<tr>
<td class="col-md-3"><a><img src='images/projects/gnet.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
<strong>VGG and GoogleNet for Image Classification and Feature Extraction</strong><br>
<div>
<ul> 
    <li>Implemented VGG and GoogleNet (Inceptionv1) image classification for training, inference and feature extraction.
    </li>
    <li>Designed a modified Inception network for training on low resolution dataset from scratch (<a href='https://github.com/conan7882/GoogLeNet-Inception#train-the-network-from-scratch-on-cifar-10' target='_blank'>achieved</a> 93.64% accuracy on CIFAR-10 testing set).
    </li>
</ul>
</div>
    
[<a href='https://github.com/conan7882/VGG-cifar' target='_blank'>VGG</a>] [<a href='https://github.com/conan7882/GoogLeNet-Inception' target='_blank'>GoogleNet</a>]<br>
    
</td>
</tr>



<tr>
<td class="col-md-3"><a><img src='images/projects/styletrans.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Image and Video Style Transfer using Fast Style Transfer and Neural Style</strong><br>
    <div>
    <ul>
    <li>
    Implemented the fast style transfer to transfer images and videos to a specific artistic style in nearly real-time. 
    </li>
    <li>
    Implemented the neural style transfer for image style transfer.
    </li>

    </ul>
    </div>
    
    [<a href='http://github.com/conan7882/fast-style-transfer' target='_blank'>fast style</a>] [<a href='https://github.com/conan7882/art_style_transfer_TensorFlow' target='_blank'>neural style</a>]<br>
</td>
</tr>


<!--<tr>
<td class="col-md-3"><a><img src='images/projects/tensorproj.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Other TensorFlow Projects</strong><br>
    There are some other projects based on TensorFlow framework, including<br>
    
<div>
<ul> 
    <li>Implementation of Deep Convolutional Generative Adversarial Networks and experiments on MNIST and CIFAR10 dataset [<a href='http://github.com/conan7882/tensorflow-DCGAN' target='_blank'>code</a>] </li> <li>Fully Convolutional Network for image segmentation [<a href='http://github.com/conan7882/tensorflow-FCN' target='_blank'>code</a>] </li> <li>Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG) [<a href='http://github.com/conan7882/VGG-tensorflow' target='_blank'>code</a>] </li>
</ul>
</div>
    <br>
    
</td>
</tr>-->



<!--<tr>
<td class="col-md-3"><a><img src='images/projects/courseproj.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Selected Course Projects</strong><br>
    I have done several course projects at NCSU, including<br>
    
<div>
<ul> 
    <li>Leaf classification based on visual features using PCA and k-means (2011) </li> <li>Face recognition based on eigenface using multilayer perceptron (MLP) (2012) </li> <li>Human activity recognition using hidden Markov model (HMM) (2015) </li>
</ul>
</div>
<br>
    
</td>
</tr>-->



</table>


## <i class="fa fa-chevron-right"></i> Research Experience
<table class="table table-hover">


<tr>
<td class="col-md-3"><a><img src='images/projects/forams.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>A Visual System for Autonomous Foraminifera Identification</strong><br>  
    Foraminifera are single-celled organisms with shells, which are useful in petroleum exploration, biostratigraphy, paleoecology and paleobiogeography.
    We developed an automated system for identification of foraminifera species to reduce the human efforts on manually picking thousands of samples from ocean sediments.
    We also created a foraminifera image dataset and proposed novel robust edge detection algorithms on this dataset.
    <br>
    
[<a href='javascript: none' 
    onclick='$("#forams").toggle()'>details</a>] [<a href='https://research.ece.ncsu.edu/aros/foram-identification/' target='_blank'>project page</a>] <br>
    
<div id="forams" style="text-align: justify; display: none" markdown="1">
<ul> 
    <li>
    	Leaded the creation of a foraminifera image dataset containing 1437 samples and 457 manually segmentation samples.
    </li>
    <li>
    	Created synthetic images refined by GANs for data augmentation.
    </li>
    <li>
    	Developed a coarse-to-fine fully convolutional edge detection network which iteratively applies edge detection modules on predicted edge maps.
    </li>
    <li>
    Achieved 0.91 edge F1 score on the foraminifera dataset for finding vague edges between foraminifera chambers with similar texture.
    </li>
    <li>
    	Designed a topology-based metric to measure the structural difference between two edge maps. 
    </li>
    <li> 
    Developed a training process utilizing the topological metric to train an edge detection network which focuses on preserving topological structures of edges. 
    </li>
    <li>
    	Improved edge F1 score from 0.91 to 0.93 and segmentation IoU from 0.80 to 0.82.
    </li>
    <li>
    	Built a transfer learning process for identification of six foraminifera species using features extracted from pre-trained VGG, Inception and ResNet.
    </li>
</ul>
</div>
</td>
</tr>
<tr>
<td class="col-md-3"><a><img src='images/projects/car.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Robust Traffic Scenes Obstacle Detection and Image Segmentation</strong><br>
    We proposed a persistent homology based image segmentation framework which is robust to image qualities and parameter selection. The application areas for this framework include autonomous driving systems and segmentation of natural and biological images.
    <br> 
    
[<a href='javascript: none' 
    onclick='$("#consensus").toggle()'>details</a>] [<a href='https://conan7882.github.io/data/posters/tda-segmentation.pdf' target='_blank'>presentation</a>]
    <br>
    
<div id="consensus" style="text-align: justify; display: none" markdown="1">
<ul> 
    <li>Proposed a persistent homology based image segmentation framework which is robust to image qualities and parameter selection.</li>
    <li>Designed a pipeline for traffic scene obstacle detection based on this framework by extracting persistence regions in occupancy grids computed from disparity maps.</li>
    <li>Demonstrated that the traffic scene obstacle detection pipeline is robust to input image quality through experiments on <a href='http://www.cvlibs.net/datasets/kitti/' target='_blank'>KITTI dataset</a>.</li>
	<li>Designed a consensus-based image segmentation based on this framework for robustly extracting consensus information from a segmentation result set generated by different segmentation algorithms.</li>
	<li>Achieved better performance over a wide range of parameters than any input algorithm with its best parameter setting on Berkeley Segmentation Database.</li>
</ul>
</div>
</td>
</tr>


<!--<tr>
<td class="col-md-3"><a><img src='images/projects/car.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Autonomous Car</strong><br>
    The objective of this research is the development of a mathematical framework that enables the identification, characterization and matching of patterns in imaging data with certain guarantees.<br>
    
    
[<a href='javascript: none' 
    onclick='$("#car").toggle()'>details</a>] <br>
    
<div id="car" style="text-align: justify; display: none" markdown="1">
<ul> 
    <li>Improved the robustness of obstacle segmentation in outdoor scenes by using topological persistence analysis on an obstacle probability map.</li> <li>Computed the semantic segmentation of outdoor scenes based on the robust obstacle segmentation and visual features using Markov random field (MRF).</li> <li>Tracking</li>
</ul>
</div>
</td>
</tr>-->

<tr>
<td class="col-md-3"><a><img src='images/projects/newspaper.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Exploring Victorian Illustrated Newspapers Data through Computer Vision Techniques</strong><br>
    The aim of this project is to answer how can computer vision and image processing techniques be adapted for large-scale interpretation of historical materials.
<!--    The aim of this project is to suggest how computer vision techniques can reveal large-scale patterns in the visual language of the nineteenth century, opening further research questions about historical visual culture and graphic knowledge.-->
    We applied several computer vision techniques on a set of nineteenth-century illustrated British newspapers to explore and test the feasibility of these techniques for analyzing large collections of these periodical illustrations.
    <br>
    
    
[<a href='javascript: none' 
    onclick='$("#newspaper").toggle()'>details</a>] [<a href='https://ncna.dh.chass.ncsu.edu/' target='_blank'>project page</a>] <br>
    
<div id="newspaper" style="text-align: justify; display: none" markdown="1">
<ul> 
	<li>Created a Victorian newspaper illustration dataset by extracting illustration regions from scanned newspaper pages with high accuracy.</li>
	<li>Developed a Fourier transform based feature to distinguish line engravings and halftone images for tracking the presence of halftone images in late nineteenth-century British newspapers.</li>
    <li>Extracted specific scenes such as portraits, crowds, buildings and weather charts using k-means, KNN and SVM based on GIST descriptor.</li>
</ul>
</div>
</td>
</tr>


<tr>
<td class="col-md-3"><a><img src='images/projects/registration.png'  onerror="this.onerror=null;this.src='images/projects/alt.jpg';"/></a> </td>
<td>
    <strong>Non-Rigid Image Registration with Uncertainty Analysis</strong><br>
    We proposed a novel non-rigid image registration methodology which can be applied to medical images as well as natural images.
    We also provided the uncertainty bounds to characterize the registration accuracy over the entire image domain. <br>
    
    
[<a href='javascript: none' 
    onclick='$("#registration").toggle()'>details</a>] [<a href='https://conan7882.github.io/data/posters/deformation_poster.pdf' target='_blank'>poster</a>]<br>
    
<div id="registration" style="text-align: justify; display: none" markdown="1">
<ul> 
    <li>Developed a topological-based correspondence point matching algorithm under a Lipschitz non-rigid deformation with zero false negative rate and high precision.</li>
    <li>Extended the point matching to region registration by solving a graph matching problem with geometric constraints.</li>
    <li>Developed an approach to quantify the uncertainty of the region registration.</li>
</ul>
</div>
</td>
</tr>



</table>


## <i class="fa fa-chevron-right"></i>Publications 

<!-- <a href="https://github.com/bamos/cv/blob/master/publications/conference.bib"><i class="fa fa-code-fork" aria-hidden="true"></i></a> -->

<!-- <a href="https://scholar.google.com/citations?user=" class="btn btn-primary" style="padding: 0.3em;">
  <i class="ai ai-google-scholar"></i> Google Scholar
</a> -->

<table class="table table-hover">

<tr>
<td>
    <strong>Automated species-level identification of planktic foraminifera using convolutional neural networks, with comparison to human performance</strong><br>
    R. Mitra, T. Marchitto, <strong>Q. Ge</strong>, B. Zhong, B. Kanakiya, M.S. Cook, J.S. Fehrenbacher, J.D. Ortiz, A. Tripati and E. Lobaton<br>
    Marine Micropaleontology 2019<br>
    [1] 
[<a href='javascript: none'
    onclick='$("#abs_2019foram").toggle()'>abs</a>] [<a href='https://www.sciencedirect.com/science/article/pii/S0377839818301105' target='_blank'>link</a>] <br>
    
<div id="abs_2019foram" style="text-align: justify; display: none" markdown="1">
Picking foraminifera from sediment samples is an essential, but repetitive and low-reward task that is well-suited for automation. The first step toward building a picking robot is the development of an automated identification system. We use machine learning techniques to train convolutional neural networks (CNNs) to identify six species of extant planktic foraminifera that are widely used by paleoceanographers, and to distinguish the six species from other taxa. We employ CNNs that were previously built and trained for image classification. Foraminiferal training and identification use reflected light microscope digital images taken at 16 different illumination angles using a light-emitting diode (LED) ring. Overall machine accuracy, as a combination of precision and recall, is better than 80% even with limited training. We compare machine performance to that of human pickers (six experts and five novices) by tasking each with the identification of 540 specimens based on images. Experts achieved comparable precision but poorer recall relative to the machine, with an average accuracy of 63%. Novices scored lower than experts on both precision and recall, for an overall accuracy of 53%. The machine achieved fairly uniform performance across the six species, while participants' scores were strongly species-dependent, commensurate with their past experience and expertise. The machine was also less sensitive to specimen orientation (umbilical versus spiral views) than the humans. These results demonstrate that our approach can provide a versatile ‘brain’ for an eventual automated robotic picking system.
</div>

</td>
</tr>

<tr>
<td>
    <strong>Image Analytics and the Nineteenth-Century Illustrated Newspaper</strong><br>
    P. Fyfe and <strong>Q. Ge</strong><br>
    Journal of Cultural Analytics 2018<br>
    [2] 
[<a href='http://culturalanalytics.org/2018/10/image-analytics-and-the-nineteenth-century-illustrated-newspaper/' target='_blank'>link</a>] <br>
    
</td>
</tr>

<tr>
<td>
    <strong>Obstacle Detection in Outdoor Scenes based on Multi-Valued Stereo Disparity Maps</strong><br>
    <strong>Q. Ge</strong> and E. Lobaton<br>
    IEEE Symp. Series Comput. Intell. (SSCI) 2017<br>
    [3] 
[<a href='javascript: none'
    onclick='$("#abs_qian2017car").toggle()'>abs</a>] [<a href='http://conan7882.github.io/data/papers/qian-ssci2017-car.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_qian2017car" style="text-align: justify; display: none" markdown="1">
In this paper, we propose a methodology for robust obstacle detection in outdoor scenes for autonomous driving applications using a multi-valued stereo disparity approach. Traditionally, disparity maps computed from stereo pairs only provide a single estimated disparity value for each pixel. However, disparity computation suffers heavily from reflections, lack of texture and repetitive patterns of objects. This may lead to wrong estimates, which can introduce some bias on obstacle detection approaches that make use of the disparity map. To overcome this problem, instead of a single-valued disparity estimation, we propose making use of multiple candidates per pixel. The candidates are selected from a statistical analysis that characterizes the performance of the underlying matching cost function based on two metrics: The number of candidates extracted, and the distance from these candidates to the true disparity value. Then, we construct an aggregate occupancy map in u-disparity space from which obstacle detection is obtained. Experiments show that our approach can recover the correct structure of obstacles on the scene when traditional estimation approaches fail.
</div>

</td>
</tr>


<tr>
<td>
    <strong>Coarse-to-Fine Foraminifera Image Segmentation through 3D and Deep Features</strong><br>
    <strong>Q. Ge</strong>, B. Zhong, B. Kanakiya, R. Mitra, T. Marchitto, and E. Lobaton<br>
    IEEE Symp. Series Comput. Intell. (SSCI) 2017<br>
    [4] 
[<a href='javascript: none'
    onclick='$("#abs_qian2017foramsseg").toggle()'>abs</a>] [<a href='http://conan7882.github.io/data/papers/qian-ssci2017-foramsseg.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_qian2017foramsseg" style="text-align: justify; display: none" markdown="1">
Foraminifera are single-celled marine organisms, which are usually less than 1 mm in diameter. One of the most common tasks associated with foraminifera is the species identification of thousands of foraminifera contained in rock or ocean sediment samples, which can be a tedious manual procedure. Thus an automatic visual identification system is desirable. Some of the primary criteria for foraminifera species identification come from the characteristics of the shell itself. As such, segmentation of chambers and apertures in foraminifera images would provide powerful features for species identification. Nevertheless, none of the existing image-based, automatic classification approaches make use of segmentation, partly due to the lack of accurate segmentation methods for foraminifera images. In this paper, we propose a learning-based edge detection pipeline, using a coarse-to-fine strategy, to extract the vague edges from foraminifera images for segmentation using a relatively small training set. The experiments demonstrate our approach is able to segment chambers and apertures of foraminifera correctly and has the potential to provide useful features for species identification and other applications such as morphological study of foraminifera shells and foraminifera dataset labeling.
</div>

</td>
</tr>


<tr>
<td>
    <strong>A Comparative Study of Image Classification Algorithms for Foraminifera Identification</strong><br>
    B. Zhong, <strong>Q. Ge</strong>, B. Kanakiya, R. Mitra, T. Marchitto, and E. Lobaton<br>
    IEEE Symp. Series Comput. Intell. (SSCI) 2017<br>
    [5] 
[<a href='javascript: none'
    onclick='$("#abs_boxuan2017foramsclassify").toggle()'>abs</a>] [<a href='http://conan7882.github.io/data/papers/boxuan-ssci2017-foramsclassify.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_boxuan2017foramsclassify" style="text-align: justify; display: none" markdown="1">
Identifying Foraminifera (or forams for short) is essential for oceanographic and geoscience research as well as petroleum exploration. Currently, this is mostly accomplished using trained human pickers, routinely taking weeks or even months to accomplish the task. In this paper, a foram identification pipeline is proposed to automatic identify forams based on computer vision and machine learning techniques. A microscope based image capturing system is used to collect a labelled image data set. Various popular image classification algorithms are adapted to this specific task and evaluated under various conditions. Finally, the potential of a weighted cross-entropy loss function in adjusting the trade-off between precision and recall is tested. The classification algorithms provide competitive results when compared to human experts labeling of the data set.
</div>

</td>
</tr>


<tr>
<td>
    <strong>Consensus-Based Image Segmentation via Topological Persistence</strong><br>
    <strong>Q. Ge</strong> and E. Lobaton<br>
    IEEE Conf. on Comput. Vis. Pattern Recognit. Workshops (CVPRW) 2016<br>
    [6] 
[<a href='javascript: none'
    onclick='$("#abs_qian2016consensus").toggle()'>abs</a>] [<a href='http://conan7882.github.io/data/papers/qian-cvprw2017-consensus.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_qian2016consensus" style="text-align: justify; display: none" markdown="1">
Image segmentation is one of the most important low-level operation in image processing and computer vision. It is unlikely for a single algorithm with a fixed set of parameters to segment various images successfully due to variations between images. However, it can be observed that the desired segmentation boundaries are often detected more consistently than other boundaries in the output of state-of-the-art segmentation results. In this paper, we propose a new approach to capture the consensus of information from a set of segmentations generated by varying parameters of different algorithms. The probability of a segmentation curve being present is estimated based on our probabilistic image segmentation model. A connectivity probability map is constructed and persistent segments are extracted by applying topological persistence to the probability map. Finally, a robust segmentation is obtained with the detection of certain segmentation curves guaranteed. The experiments demonstrate our algorithm is able to consistently capture the curves present within the segmentation set.
</div>

</td>
</tr>


<tr>
<td>
    <strong>Robust Multi-Target Tracking in Outdoor Traffic Scenarios via Persistence Topology based Robust Motion Segmentation</strong><br>
    S. Chattopadhyay, <strong>Q. Ge</strong>, C. Wei, and E. Lobaton<br>
    IEEE Global Conf. Signal Inf. Process. (GlobalSIP) 2015<br>
    [7] 
[<a href='javascript: none'
    onclick='$("#abs_somrita2015car").toggle()'>abs</a>] [<a href='http://conan7882.github.io/data/papers/somrita-globalsip2017-car.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_somrita2015car" style="text-align: justify; display: none" markdown="1">
In this paper, we present a motion segmentation based robust multi-target tracking technique for on-road obstacles. Our approach uses depth imaging information, and integrates persistence topology for segmentation and min-max network flow for tracking. To reduce time as well as computational complexity, the max flow problem is solved using a dynamic programming algorithm. We classify the sensor reading into regions of stationary and moving parts by aligning occupancy maps obtained from the disparity images and then, incorporate Kalman filter in the network flow algorithm to track the moving objects robustly. Our algorithm has been tested on several real-life stereo datasets and the results show that there is an improvement by a factor of three on robustness when comparing performance with and without the topological persistent detections. We also perform measurement accuracy of our algorithm using popular evaluation metrics for segmentation and tracking, and the results look promising.
</div>

</td>
</tr>


<tr>
<td>
    <strong>Robust Obstacle Segmentation based on Topological Persistence in Outdoor Traffic Scenes</strong><br>
    C. Wei, <strong>Q. Ge</strong>, S. Chattopadhyay, and E. Lobaton<br>
    IEEE Symp. Series Comput. Intell. (SSCI) 2014<br>
    [8] 
[<a href='javascript: none'
    onclick='$("#abs_chunpeng2014car").toggle()'>abs</a>] [<a href='http://conan7882.github.io/data/papers/chunpeng-ssci2014-car.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_chunpeng2014car" style="text-align: justify; display: none" markdown="1">
In this paper, a new methodology for robust seg- mentation of obstacles from stereo disparity maps in an on- road environment is presented. We first construct a probability of the occupancy map using the UV-disparity methodology. Traditionally, a simple threshold has been applied to segment obstacles from the occupancy map based on the connectivity of the resulting regions; however, this outcome is sensitive to the choice of parameter value. In our proposed method, instead of simple thresholding, we perform a topological persistence analysis on the constructed occupancy map. The topological framework hierarchically encodes all possible segmentation results as a function of the threshold, thus we can identify the regions that are most persistent. This leads to a more robust segmentation. The approach is analyzed using real stereo image pairs from standard datasets.
</div>

</td>
</tr>


<tr>
<td>
    <strong>Manifold Learning Approach to Curve Identification with Applications to Footprint Segmentation</strong><br>
    N. Lokare, <strong>Q. Ge</strong>, W. Snyder, Z. Jewell, S. Allibhai, and E. Lobaton<br>
    IEEE Symp. Series Comput. Intell. (SSCI) 2014<br>
    [9] 
[<a href='javascript: none'
    onclick='$("#abs_namita2014footprint").toggle()'>abs</a>] [<a href='http://conan7882.github.io/data/papers/namita-ssci2014footprint.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_namita2014footprint" style="text-align: justify; display: none" markdown="1">
Recognition of animals via images of their foot- prints is a non-invasive technique recently adopted by researchers interested in monitoring endangered species. One of the challenges that they face is the extraction of features from these images, which are required for this approach. These features are points along the boundary curve of the footprints. In this paper, we propose an innovative technique for extracting these curves from depth images. We formulate the problem of identification of the boundary of the footprint as a pattern recognition problem of a stochastic process over a manifold. This methodology has other applications on segmentation of biological tissue for medical applications and tracking of extreme weather patterns. The problem of pattern identification in the manifold is posed as a shortest path problem, where the path with the smallest cost is identified as the one with the highest likelihood to belong to the stochastic process. Our methodology is tested in a new dataset of normalized depth images of tiger footprints with ground truth selected by experts in the field.
</div>

</td>
</tr>


<tr>
<td>
    <strong>Non-Rigid Image Registration under Non-Deterministic Deformation Bounds</strong><br>
    <strong>Q. Ge</strong>, N. Lokare, and E. Lobaton<br>
    10th International Symposium on Medical Information Processing and Analysis 2014<br>
    [10] 
[<a href='javascript: none'
    onclick='$("#abs_qian2014registration").toggle()'>abs</a>] [<a href='http://conan7882.github.io/data/papers/qian-sipim2014-registration.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_qian2014registration" style="text-align: justify; display: none" markdown="1">
Image registration aims to identify the mapping between corresponding locations in an anatomic structure. Most traditional approaches solve this problem by minimizing some error metric. However, they do not quantify the uncertainty behind their estimates and the feasibility of other solutions. In this work, it is assumed that two images of the same anatomic structure are related via a Lipschitz non-rigid deformation (the registration map). An approach for identifying point correspondences with zero false-negative rate and high precision is introduced under this assumption. This methodology is then extended to registration of regions in an image which is posed as a graph matching problem with geometric constraints. The outcome of this approach is a homeomorphism with uncertainty bounds characterizing its accuracy over the entire image domain. The method is tested by applying deformation maps to the LPBA40 dataset.
</div>

</td>
</tr>


</table>


## <i class="fa fa-chevron-right"></i> Teaching Experience
<table class="table table-hover">
<tr>
  <td class='col-md-1'>F2018</td>
  <td><strong>Neural Networks</strong> (NCSU ECE 542), TA</td>
</tr>
<tr>
  <td class='col-md-1'>F2016</td>
  <td><strong>Applications of Graphs and Graphical Models</strong> (NCSU ECE/CSC 792), TA</td>
</tr>
<tr>
  <td class='col-md-1'>F2015</td>
  <td><strong>Computer Systems Programming</strong> (NCSU ECE 209), TA</td>
</tr>
</table>
